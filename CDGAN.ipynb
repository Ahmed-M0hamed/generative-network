{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7eb585",
   "metadata": {},
   "source": [
    "# CGAN => conditional gan \n",
    "### this network solved the problem of randomness for the DGAN network by giving the network sense of the outputs by adding one-hot-encoding label vector to both generator and discriminator and give us the ability to control what the network generate so I used this architecture \n",
    "\n",
    "![CGAN](images/CGAN.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os \n",
    "from tensorflow.keras.layers import Dense , Conv2D ,concatenate, Conv2DTranspose , Flatten , Reshape  , BatchNormalization , Activation, Flatten ,LeakyReLU\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import math \n",
    "import os \n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4788d",
   "metadata": {},
   "source": [
    "# Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0447d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model) : \n",
    "    def __init__(self) : \n",
    "        super().__init__()  \n",
    "        self.dense_1 = Dense(7*7*128 ) \n",
    "        self.reshape = Reshape((7,7,128))\n",
    "\n",
    "        \n",
    "        self.sequential = Sequential([\n",
    "             BatchNormalization() ,\n",
    "             Activation('relu') , \n",
    "             Conv2DTranspose(128 , 5 , strides = 2 , padding = 'same') , \n",
    "             BatchNormalization() ,\n",
    "             Activation('relu') ,\n",
    "             Conv2DTranspose(64 , 5 , strides = 2  , padding = 'same') ,\n",
    "             BatchNormalization() ,\n",
    "             Activation('relu') ,\n",
    "             Conv2DTranspose(32 , 5   , padding = 'same') ,\n",
    "             BatchNormalization() ,\n",
    "             Activation('relu') ,\n",
    "             Conv2DTranspose(1 , 5  , padding = 'same') , \n",
    "             Activation('sigmoid')\n",
    "        ])\n",
    "        \n",
    "    def call(self , inputs ) :\n",
    "        # we get noise vector and one hot label vector as inputs \n",
    "        X_vector , one_hot_vector = inputs\n",
    "        X = concatenate([X_vector , one_hot_vector] , axis = 1) \n",
    "        X = self.reshape(self.dense_1(X ))\n",
    "        return self.sequential(X)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1c18c",
   "metadata": {},
   "source": [
    "# Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model) : \n",
    "    def __init__(self) : \n",
    "        super().__init__() \n",
    "        self.label_dense = Dense(7*7*16) \n",
    "        self.reshape = Reshape((28 ,28 ,1)) \n",
    "        self.sequential = Sequential([\n",
    "            LeakyReLU(.2), \n",
    "            Conv2D(32 , 5 , padding ='same' , strides = 2) , \n",
    "            LeakyReLU(.2),\n",
    "            Conv2D(64 , 5 , padding ='same' , strides = 2) ,\n",
    "            LeakyReLU(.2),\n",
    "            Conv2D(128 , 5 , padding ='same' , strides = 2) ,\n",
    "            LeakyReLU(.2),\n",
    "            Conv2D(256 , 5 , padding ='same'  ) ,\n",
    "            Flatten() , \n",
    "            Dense(1) , \n",
    "            Activation('sigmoid') \n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def call(self, inputs ) : \n",
    "        image ,one_hot_vector = inputs \n",
    "        # we do some sort of embedding to the label vector and reshape it to the image shape \n",
    "        lebel_embed = self.label_dense(one_hot_vector ) \n",
    "        label_embed = self.reshape(lebel_embed) \n",
    "        X = concatenate([image , label_embed]) \n",
    "        return self.sequential(X)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca4575",
   "metadata": {},
   "source": [
    "# Adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3702ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversarial(tf.keras.Model) : \n",
    "    def __init__(self , generator , discriminator) : \n",
    "        super().__init__()\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator\n",
    "        \n",
    "    def call(self , inputs ) : \n",
    "        \n",
    "        noise , labels = inputs \n",
    "        # first we pass noise and labels to generator \n",
    "        images = self.generator((noise , labels)) \n",
    "        # and we pass the generator output and labels to discriminator\n",
    "        return self.discriminator((images , labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d485d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, data, params):\n",
    "    \n",
    "    generator, discriminator, adversarial = models\n",
    "    x_train, y_train = data\n",
    "    batch_size, latent_size, train_steps, num_labels, model_name = params\n",
    "    save_interval = 500\n",
    "    # noise vector to see how the generator output evolves during training\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "    # one-hot label the noise will be conditioned to\n",
    "    noise_class = np.eye(num_labels)[np.arange(0, 16) % num_labels]\n",
    "    # number of elements in train dataset\n",
    "    train_size = x_train.shape[0]\n",
    "\n",
    "    print(model_name,\n",
    "          \"Labels for generated images: \",\n",
    "          np.argmax(noise_class, axis=1))\n",
    "\n",
    "    for i in range(train_steps):\n",
    "\n",
    "\n",
    "        ##### train the discriminator for 1 batch\n",
    "        # randomly pick real images from dataset\n",
    "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "        real_images = x_train[rand_indexes]\n",
    "        # corresponding one-hot labels of real images\n",
    "        real_labels = y_train[rand_indexes]\n",
    "        # generate fake images from noise using generator\n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0,\n",
    "                                  1.0,\n",
    "                                  size=[batch_size, latent_size])\n",
    "        # assign random one-hot labels\n",
    "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels,\n",
    "                                                          batch_size)]\n",
    "\n",
    "        # generate fake images conditioned on fake labels\n",
    "        fake_images = generator.predict((noise, fake_labels))\n",
    "        # real + fake images = 1 batch of train data\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        # real + fake one-hot labels = 1 batch of train one-hot labels\n",
    "        labels = np.concatenate((real_labels, fake_labels))\n",
    "\n",
    "        # label real and fake images\n",
    "        # real images label is 1.0\n",
    "        y = np.ones([2 * batch_size, 1])\n",
    "        # fake images label is 0.0\n",
    "        y[batch_size:, :] = 0.0\n",
    "        # train discriminator network, log the loss and accuracy\n",
    "        loss, acc = discriminator.train_on_batch((x, labels), y)\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
    "\n",
    "        ######## train the adversarial network for 1 batch\n",
    "        # since the discriminator weights are frozen in \n",
    "        # adversarial network only the generator is trained\n",
    "        # generate noise using uniform distribution        \n",
    "        noise = np.random.uniform(-1.0,\n",
    "                                  1.0,\n",
    "                                  size=[batch_size, latent_size])\n",
    "        # assign random one-hot labels\n",
    "        fake_labels = np.eye(num_labels)[np.random.choice(num_labels,\n",
    "                                                          batch_size)]\n",
    "        # label fake images as real or 1.0\n",
    "        y = np.ones([batch_size, 1])\n",
    "        # train the adversarial network \n",
    "        loss, acc = adversarial.train_on_batch((noise, fake_labels), y)\n",
    "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n",
    "        print(log)\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            # plot generator images on a periodic basis\n",
    "            plot_images(generator,\n",
    "                        noise_input=noise_input,\n",
    "                        noise_class=noise_class,\n",
    "                        show=False,\n",
    "                        step=(i + 1),\n",
    "                        model_name=model_name)\n",
    "    \n",
    "    # save the model after training the generator\n",
    "    # the trained generator can be reloaded for \n",
    "    # future MNIST digit generation\n",
    "    generator.save(model_name + \".h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(generator,\n",
    "                noise_input,\n",
    "                noise_class,\n",
    "                show=False,\n",
    "                step=0,\n",
    "                model_name=\"gan\"):\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    filename = os.path.join(model_name, \"%05d.png\" % step)\n",
    "    images = generator.predict([noise_input, noise_class])\n",
    "    print(model_name , \" labels for generated images: \", np.argmax(noise_class, axis=1))\n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    num_images = images.shape[0]\n",
    "    image_size = images.shape[1]\n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(images[i], [image_size, image_size])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00147fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_models():\n",
    "    # load MNIST dataset\n",
    "    (x_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "    # reshape data for CNN as (28, 28, 1) and normalize\n",
    "    image_size = x_train.shape[1]\n",
    "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "\n",
    "    num_labels = np.amax(y_train) + 1\n",
    "    y_train = to_categorical(y_train)\n",
    "\n",
    "    model_name = \"cgan_mnist\"\n",
    "    # network parameters\n",
    "    # the latent or z vector is 100-dim\n",
    "    latent_size = 100\n",
    "    batch_size = 64\n",
    "    train_steps = 40000\n",
    "    lr = 2e-4\n",
    "    decay = 6e-8\n",
    "    input_shape = (image_size, image_size, 1)\n",
    "    label_shape = (num_labels, )\n",
    "\n",
    "    \n",
    "\n",
    "    discriminator = Discriminator()\n",
    "    # [1] or original paper uses Adam, \n",
    "    # but discriminator converges easily with RMSprop\n",
    "    optimizer = RMSprop(lr=lr, decay=decay)\n",
    "    discriminator.compile(loss='binary_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "   \n",
    "    generator = Generator()\n",
    "\n",
    "\n",
    "    # build adversarial model = generator + discriminator\n",
    "    optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)\n",
    "    # freeze the weights of discriminator during adversarial training\n",
    "    discriminator.trainable = False\n",
    "    adversarial = Adversarial(generator,discriminator)\n",
    "    \n",
    "    adversarial.compile(loss='binary_crossentropy',\n",
    "                        optimizer=optimizer,\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "    # train discriminator and adversarial networks\n",
    "    models = (generator, discriminator, adversarial)\n",
    "    data = (x_train, y_train)\n",
    "    params = (batch_size, latent_size, train_steps, num_labels, model_name)\n",
    "    train(models, data, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6c5ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "build_and_train_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
