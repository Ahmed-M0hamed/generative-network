{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7eb585",
   "metadata": {},
   "source": [
    "# CGAN => conditional gan \n",
    "### this network solved the problem of randomness for the DGAN network by giving the network sense of the outputs by adding one-hot-encoding label vector to both generator and discriminator and give us the ability to control what the network generate so I used this architecture \n",
    "\n",
    "![CGAN](images/CGAN.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os \n",
    "from tensorflow.keras.layers import Dense , Conv2D ,concatenate, Conv2DTranspose , Flatten , Reshape  , BatchNormalization , Activation, Flatten ,LeakyReLU\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import math \n",
    "import os \n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4788d",
   "metadata": {},
   "source": [
    "# Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0447d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model) : \n",
    "    def __init__(self) : \n",
    "        super().__init__()  \n",
    "        self.dense_1 = Dense(7*7*128 ) \n",
    "        self.reshape = Reshape((7,7,128))\n",
    "\n",
    "        \n",
    "        self.sequential = Sequential([\n",
    "             BatchNormalization() ,\n",
    "             Activation('relu') , \n",
    "             Conv2DTranspose(128 , 5 , strides = 2 , padding = 'same' , use_bias = False) , \n",
    "             BatchNormalization() ,\n",
    "             Activation('relu') ,\n",
    "             Conv2DTranspose(64 , 5 , strides = 2  , padding = 'same' ,use_bias = False) ,\n",
    "             BatchNormalization() ,\n",
    "             Activation('relu') ,\n",
    "             Conv2DTranspose(32 , 5   , padding = 'same' , use_bias = False) ,\n",
    "             BatchNormalization() ,\n",
    "             Activation('relu') ,\n",
    "             Conv2DTranspose(1 , 5  , padding = 'same' , use_bias = False) , \n",
    "             Activation('sigmoid')\n",
    "        ])\n",
    "        \n",
    "    def call(self , inputs ) :\n",
    "        # we get noise vector and one hot label vector as inputs \n",
    "        X_vector , one_hot_vector = inputs\n",
    "#         print(one_hot_vector.shape)\n",
    "        X = concatenate([X_vector , one_hot_vector] , axis = 1) \n",
    "        X = self.reshape(self.dense_1(X ))\n",
    "        return self.sequential(X)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1c18c",
   "metadata": {},
   "source": [
    "# Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model) : \n",
    "    def __init__(self) : \n",
    "        super().__init__() \n",
    "        self.label_dense = Dense(7*7*16) \n",
    "        self.reshape = Reshape((28 ,28 ,1)) \n",
    "        self.sequential = Sequential([\n",
    "            LeakyReLU(.2), \n",
    "            Conv2D(32 , 5 , padding ='same' , strides = 2) , \n",
    "            LeakyReLU(.2),\n",
    "            Conv2D(64 , 5 , padding ='same' , strides = 2) ,\n",
    "            LeakyReLU(.2),\n",
    "            Conv2D(128 , 5 , padding ='same' , strides = 2) ,\n",
    "            LeakyReLU(.2),\n",
    "            Conv2D(256 , 5 , padding ='same'  ) ,\n",
    "            Flatten() , \n",
    "            Dense(1) , \n",
    "            Activation('sigmoid') \n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def call(self, inputs ) : \n",
    "        image ,one_hot_vector = inputs \n",
    "        # we do some sort of embedding to the label vector and reshape it to the image shape \n",
    "        lebel_embed = self.label_dense(one_hot_vector ) \n",
    "        label_embed = self.reshape(lebel_embed) \n",
    "        X = concatenate([image , label_embed]) \n",
    "        return self.sequential(X)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d485d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images , labels , generator , discriminator  , loss_fn , gen_optimizer , dis_optimizer):\n",
    "    noise = tf.random.normal([64, 100])\n",
    "    fake_labels = np.eye(10)[np.random.choice(10,64)]\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator((noise , fake_labels), training=True)\n",
    "\n",
    "        real_output = discriminator((images , labels ) , training=True)\n",
    "        fake_output = discriminator((generated_images , fake_labels) , training=True)\n",
    "\n",
    "        gen_loss = loss_fn(fake_output , tf.ones_like(fake_output))\n",
    "        disc_real_loss = loss_fn(real_output, tf.ones_like(real_output)) \n",
    "        disc_fake_loss = loss_fn(fake_output , tf.zeros_like(fake_output))\n",
    "        disc_loss = disc_real_loss + disc_fake_loss\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    dis_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(generator,\n",
    "                noise_input,\n",
    "                noise_class,\n",
    "                show=False,\n",
    "                step=0,\n",
    "                model_name=\"gan\"):\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "    filename = os.path.join(model_name, \"%05d.png\" % step)\n",
    "    images = generator.predict([noise_input, noise_class])\n",
    "    print(model_name , \" labels for generated images: \", np.argmax(noise_class, axis=1))\n",
    "    plt.figure(figsize=(2.2, 2.2))\n",
    "    num_images = images.shape[0]\n",
    "    image_size = images.shape[1]\n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(images[i], [image_size, image_size])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00147fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_models(epochs):\n",
    "    # load MNIST dataset\n",
    "    (x_train, y_train ), (_, _) = mnist.load_data()\n",
    "\n",
    "    # reshape data for CNN as (28, 28, 1) and normalize\n",
    "    x_train = np.reshape(x_train, [-1, 28, 28, 1])\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    y_train = tf.keras.utils.to_categorical(y_train , 10)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x_train , y_train) ) \n",
    "    ds = ds.shuffle(6000).batch(64)\n",
    "\n",
    "    noise_class = np.eye(10)[np.arange(0, 16) % 10]\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "    # build discriminator model\n",
    "    discriminator = Discriminator()\n",
    "    # build generator model\n",
    "    generator = Generator()\n",
    "    \n",
    "    for epoch in range(epochs) : \n",
    "        for batch_images , labels  in ds : \n",
    "            train_step(images = batch_images , labels = labels , generator = generator  , discriminator = discriminator\n",
    "            , loss_fn =cross_entropy , gen_optimizer = generator_optimizer , dis_optimizer =discriminator_optimizer )\n",
    "\n",
    "        plot_images(generator ,tf.random.normal([16, 100]), noise_class , step = epoch  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6c5ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "build_and_train_models(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33288d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
