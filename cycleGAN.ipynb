{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bbd9830",
   "metadata": {},
   "source": [
    "# cycle gan \n",
    "\n",
    "### this network came to solve the problem that we need paired of input and output to learn the distribution of the target domain so the idea of the network if we have two domains domian X , Y we will build two networks to learn the characteristics of the two domains and get generate new data so we build two generators and two discriminator  \n",
    "\n",
    "\n",
    "![cycle gan](images/cycle-gan.png) \n",
    "\n",
    "\n",
    "### first we pass the X domain data through generator to get output F(X) and we go through two steps from here 1- we pass the F(X) domain data to the discriminator to distinguish between real and fake data 2- we pass the F(X) through the other generator to reconstruct the X domain data and we do the same process with the  Y domain data  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc068623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os \n",
    "from tensorflow.keras.layers import Dense , Conv2D ,concatenate,Conv2DTranspose , Flatten , Reshape  , BatchNormalization , Activation, Flatten ,LeakyReLU\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.datasets import mnist \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import math \n",
    "import os \n",
    "import argparse\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b027b99",
   "metadata": {},
   "source": [
    "# the generator \n",
    "\n",
    "### the generator is autoencoder network which will follow this architecture \n",
    "\n",
    "![generator](images/generator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_block(tf.keras.Model) :\n",
    "    def __init__(self ,filters=16,kernel_size=3,strides=2,activation='relu',instance_norm=True) : \n",
    "        super(encoder_block , self).__init__()\n",
    "        self.instance_norm  = instance_norm\n",
    "        self.norm = InstanceNormalization()\n",
    "        if activation == 'relu' : \n",
    "            self.activation = Activation('relu')\n",
    "        else : \n",
    "            self.activation = LeakyReLU(.2)\n",
    "             \n",
    "        self.conv = Conv2D(filters , kernel_size  , strides = strides , padding = 'same' )\n",
    "        \n",
    "    def call(self , X ) : \n",
    "        if self.instance_norm :\n",
    "                X = self.norm(X)\n",
    "        X = self.activation(X)\n",
    "        return self.conv(X) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5153b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_block(tf.keras.Model): \n",
    "    def __init__(self,filters=16,kernel_size=3,strides=2,activation='relu',instance_norm=True) : \n",
    "        super().__init__()\n",
    "        self.norm = InstanceNormalization()\n",
    "        if activation == 'relu' : \n",
    "            self.activation = Activation('relu')\n",
    "        else : \n",
    "            self.activation = LeakyReLU(.2) \n",
    "        self.convT = Conv2DTranspose(filters , kernel_size , padding = 'same' , strides = strides )\n",
    "        \n",
    "    def call(self , paired_inputs) : \n",
    "        X , skip_connection = paired_inputs \n",
    "        X = self.norm(X) \n",
    "        X = self.activation(X)\n",
    "        X = self.convT(X) \n",
    "        return concatenate([X , skip_connection]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c0df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net_Network(tf.keras.Model) : \n",
    "    def __init__(self) : \n",
    "        super().__init__() \n",
    "        self.encoder_block_1 = encoder_block(32 ,  3 ,1  , activation='leaky_relu') \n",
    "        self.encoder_block_2 = encoder_block(64 , 3 ,2 , activation='leaky_relu' )\n",
    "        self.encoder_block_3 = encoder_block(128 , 3, 2 , activation='leaky_relu')\n",
    "        self.encoder_block_4 = encoder_block(256 , 3 ,2  , activation='leaky_relu') \n",
    "        \n",
    "        self.decoder_block_1 = decoder_block(128 , 3 ,2 ) \n",
    "        self.decoder_block_2 = decoder_block(64 ,3 ,2) \n",
    "        self.decoder_block_3 = decoder_block(32 , 3, 2)\n",
    "        \n",
    "        self.outputs = Conv2DTranspose(3,\n",
    "                              kernel_size=3,\n",
    "                              strides=1,\n",
    "                              activation='sigmoid',\n",
    "                              padding='same')\n",
    "        \n",
    "        \n",
    "    def call(self , X ) : \n",
    "        S_1 = self.encoder_block_1(X )\n",
    "        S_2 = self.encoder_block_2(S_1 )\n",
    "        S_3 = self.encoder_block_3(S_2) \n",
    "        S_4 = self.encoder_block_4(S_3) \n",
    "        \n",
    "        D_1 = self.decoder_block_1((S_4 ,S_3)) \n",
    "        D_2 = self.decoder_block_2((D_1 ,S_2))\n",
    "        D_3 = self.decoder_block_3((D_2 ,S_1)) \n",
    "        \n",
    "        return self.outputs(D_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f697e4",
   "metadata": {},
   "source": [
    "## Discriminator \n",
    "\n",
    "### the Discriminator will follow this architecture\n",
    "\n",
    "![Discriminator](images/discriminator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9141775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model) : \n",
    "    def __init__(self ) : \n",
    "        super().__init__() \n",
    "        self.encoder_block_1 = encoder_block(32 ,  3 , activation='leaky_relu' , instance_norm = False ) \n",
    "        self.encoder_block_2 = encoder_block(64 , 3  , activation='leaky_relu' , instance_norm = False  )\n",
    "        self.encoder_block_3 = encoder_block(128 , 3 , activation='leaky_relu' , instance_norm = False )\n",
    "        self.encoder_block_4 = encoder_block(256 , 3 , activation='leaky_relu' , instance_norm = False) \n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(1 , activation ='linear')\n",
    "         \n",
    "        \n",
    "    def call(self , X ) : \n",
    "        X = self.encoder_block_1(X)\n",
    "        X = self.encoder_block_2(X)\n",
    "        X = self.encoder_block_3(X) \n",
    "        X = self.encoder_block_4(X) \n",
    "        X = self.flatten(X) \n",
    "        return self.dense(X) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a0ca3",
   "metadata": {},
   "source": [
    "## training \n",
    "\n",
    "### we will follow some steps to train the network \n",
    "\n",
    "- we will generate the fake domain y data from domain x data \n",
    "- we will reconstruct this fake y data to x data \n",
    "- we will the same think with y domain data generate x domain and reconstruct y \n",
    "\n",
    "- pass real and fake X , y data to the discriminator to train \n",
    "\n",
    "- calculate the losses \n",
    "- calculate the gradients \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c2a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def cycle_loss(real_image, cycled_image):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    return 10 * loss1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(real_image, same_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return 10 * 0.5 * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c69efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "      return loss_obj(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be15573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data() :     \n",
    "    BUFFER_SIZE = 1000\n",
    "    BATCH_SIZE = 1\n",
    "    IMG_WIDTH = 256\n",
    "    IMG_HEIGHT = 256\n",
    "    \n",
    "    dataset = tfds.load('cycle_gan/horse2zebra',\n",
    "                              as_supervised=True)\n",
    "\n",
    "    train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n",
    "    \n",
    "    def random_crop(image):\n",
    "        cropped_image = tf.image.random_crop(\n",
    "          image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "        return cropped_image\n",
    "    \n",
    "    # normalizing the images to [-1, 1]\n",
    "    def normalize(image):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = (image / 127.5) - 1\n",
    "        return image\n",
    "    \n",
    "    def random_jitter(image):\n",
    "          # resizing to 286 x 286 x 3\n",
    "        image = tf.image.resize(image, [286, 286],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "          # randomly cropping to 256 x 256 x 3\n",
    "        image = random_crop(image)\n",
    "          # random mirroring\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        return image\n",
    "    def preprocess_image_train(image, label):\n",
    "        image = random_jitter(image)\n",
    "        image = normalize(image)\n",
    "        return image\n",
    "    \n",
    "    train_horses = train_horses.cache().map(\n",
    "    preprocess_image_train).shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "    train_zebras = train_zebras.cache().map(\n",
    "    preprocess_image_train).shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    \n",
    "    return train_horses , train_zebras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def train_step(real_x , real_y , models , optimizers  , loss_functions ) : \n",
    "    generator_x , generator_y , discriminator_x , discriminator_y = models \n",
    "    generator_x_optimizer , generator_y_optimizer ,discriminator_x_optimizer , discriminator_y_optimizer = optimizers \n",
    "    identity_loss , cycle_loss , generator_loss , discriminator_loss = loss_functions \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_y = generator_x(real_x, training=True)\n",
    "        cycled_x = generator_y(fake_y, training=True)\n",
    "\n",
    "        fake_x = generator_y(real_y, training=True)\n",
    "        cycled_y = generator_x(fake_x, training=True)\n",
    "    \n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_y(real_x, training=True)\n",
    "        same_y = generator_x(real_y, training=True)\n",
    "        \n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "        \n",
    "        gen_x_loss = generator_loss(disc_fake_y)\n",
    "        gen_y_loss = generator_loss(disc_fake_x) \n",
    "        \n",
    "        t_cycle_loss = cycle_loss(real_x , cycled_x) + cycle_loss(real_y , cycled_y ) \n",
    "        \n",
    "        t_gen_x_loss = gen_x_loss + t_cycle_loss + identity_loss(real_y , same_y)\n",
    "        t_gen_y_loss = gen_y_loss + t_cycle_loss + identity_loss(real_x , same_x) \n",
    "        \n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "        \n",
    "        generator_x_gradients = tape.gradient(t_gen_x_loss, \n",
    "                                        generator_x.trainable_variables)\n",
    "        generator_y_gradients = tape.gradient(t_gen_y_loss, \n",
    "                                        generator_y.trainable_variables)\n",
    "        \n",
    "        discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                            discriminator_x.trainable_variables)\n",
    "        discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
    "                                            discriminator_y.trainable_variables)\n",
    "        \n",
    "        generator_x_optimizer.apply_gradients(zip(generator_x_gradients , generator_x.trainable_variables))\n",
    "        generator_y_optimizer.apply_gradients(zip(generator_y_gradients , generator_y.trainable_variables))\n",
    "        \n",
    "        discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients , discriminator_x.trainable_variables))\n",
    "        discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients , discriminator_y.trainable_variables))\n",
    "        \n",
    "        return t_gen_x_loss , t_gen_y_loss , disc_x_loss , disc_y_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ab01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs) : \n",
    "    \n",
    "    generator_x = U_Net_Network()\n",
    "    generator_y = U_Net_Network() \n",
    "    discriminator_x = Discriminator()\n",
    "    discriminator_y = Discriminator()\n",
    "    models = (generator_x , generator_y , discriminator_x , discriminator_y )\n",
    "    \n",
    "    generator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5) \n",
    "    generator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5) \n",
    "    discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5) \n",
    "    discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    optimizers = (generator_x_optimizer ,generator_y_optimizer ,discriminator_x_optimizer  , discriminator_y_optimizer )\n",
    "    \n",
    "    loss_functions = (identity_loss , cycle_loss , generator_loss , discriminator_loss) \n",
    "    \n",
    "    x_ds , y_ds = data()\n",
    "    \n",
    "    for epoch in range(epochs ) :\n",
    "        gen_x_losses = []\n",
    "        gen_y_losses = []\n",
    "        disc_x_losses = []\n",
    "        disc_y_losses = []\n",
    "        for batch_x , batch_y in zip(x_ds , y_ds) : \n",
    "            total_gen_x_loss , total_gen_y_loss , disc_x_loss , disc_y_loss = train_step(batch_x , batch_y, \n",
    "                                                                                        models , optimizers, loss_functions )\n",
    "            gen_x_losses.append(total_gen_x_loss)\n",
    "            gen_y_losses.append(total_gen_y_loss)\n",
    "            disc_x_losses.append(disc_x_loss)\n",
    "            disc_y_losses.append(disc_y_loss)\n",
    "    \n",
    "        print(f'gen_x_loss : {np.mean(gen_x_losses)} , gen_y_loss : {np.mean(gen_y_losses)} , disc_x_losses :{np.mean(disc_x_losses)} , dics_y_losses :{np.mean(disc_y_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ce6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
